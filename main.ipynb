{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9RAYGPaj2d7"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for data handling, image processing, and model building\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,  BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import Sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7GR2ceglUc-"
      },
      "source": [
        "# Specify the path to the Air-Pollution-Image-Dataset-From-India-and-Nepal CSV files after extraction\n",
        "Provide the file paths for the training, validation, and testing CSV files from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxftwwVBj2eA"
      },
      "outputs": [],
      "source": [
        "# Define file paths for the dataset CSV files\n",
        "train_csv_file = 'Datasets/train_data.csv'\n",
        "val_csv_file = 'Datasets/val_data.csv'\n",
        "test_csv_file = 'Datasets/testing_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement Mixup augmentation to enhance model generalization\n",
        "def apply_mixup(images, labels, alpha=0.2):\n",
        "    batch_size = images.shape[0]\n",
        "    lam = np.random.beta(alpha, alpha, batch_size)\n",
        "    indices = np.random.permutation(batch_size)\n",
        "    \n",
        "    mixed_images = np.zeros_like(images)\n",
        "    mixed_labels = np.zeros_like(labels)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        mixed_images[i] = lam[i] * images[i] + (1 - lam[i]) * images[indices[i]]\n",
        "        mixed_labels[i] = lam[i] * labels[i] + (1 - lam[i]) * labels[indices[i]]\n",
        "    \n",
        "    return mixed_images, mixed_labels\n",
        "\n",
        "# Create a custom Sequence class to apply Mixup to the data generator\n",
        "class MixupSequence(Sequence):\n",
        "    def __init__(self, generator, alpha=0.2):\n",
        "        self.generator = generator\n",
        "        self.alpha = alpha\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.generator)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        images, labels = self.generator[index]\n",
        "        mixed_images, mixed_labels = apply_mixup(images, labels, self.alpha)\n",
        "        return mixed_images, mixed_labels\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.generator.on_epoch_end()\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        return getattr(self.generator, attr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8UvOvCsj2eB"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the dataset, including class weight calculation and data augmentation\n",
        "import cv2\n",
        "\n",
        "# Read the training, validation, and testing CSV files\n",
        "train_df = pd.read_csv(train_csv_file)\n",
        "val_df = pd.read_csv(val_csv_file)\n",
        "test_df = pd.read_csv(test_csv_file)\n",
        "\n",
        "# Compute class weights to balance the dataset\n",
        "max_count = train_df['AQI_Class'].value_counts(normalize=True).max()\n",
        "class_weights = {cls: max_count / count for cls, count in train_df['AQI_Class'].value_counts(normalize=True).items()}\n",
        "\n",
        "# Define image normalization function using ImageNet statistics\n",
        "def normalize(image):\n",
        "    image = image.astype('float32') / 255.0\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = (image - mean) / std\n",
        "    return image\n",
        "\n",
        "# Set the image directory path\n",
        "path = 'Datasets/Images'\n",
        "\n",
        "# Configure training data generator with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=normalize,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Configure validation and test data generators without augmentation\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=normalize)\n",
        "\n",
        "# Create data generators for training, validation, and testing\n",
        "train_generator = MixupSequence(\n",
        "    train_datagen.flow_from_dataframe(\n",
        "        train_df,\n",
        "        directory='Datasets/Images',\n",
        "        x_col='Filename',\n",
        "        y_col='AQI_Class',\n",
        "        target_size=(224, 224),\n",
        "        class_mode='categorical',\n",
        "        batch_size=32\n",
        "    ),\n",
        "    alpha=0.1\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    directory='Datasets/Images',\n",
        "    x_col='Filename',\n",
        "    y_col='AQI_Class',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='categorical',\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "test_generator = val_datagen.flow_from_dataframe(\n",
        "    test_df,\n",
        "    directory='Datasets/Images',\n",
        "    x_col='Filename',\n",
        "    y_col='AQI_Class',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='categorical',\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWEsPFPdkX2g"
      },
      "source": [
        "# Build and train a model for AQI class image classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIaGAxrvkXcc"
      },
      "outputs": [],
      "source": [
        "# Build and train a VGG-19 model for AQI class image classification\n",
        "# Define a helper function to create a convolutional block with specified filters and layers\n",
        "def make_layer(x, filters, num_convs, kernel_size=3, stride=1, padding='same', weight_decay=1e-4):\n",
        "    initializer = HeNormal(seed=42)  # Use Kaiming (HeNormal) initialization\n",
        "    for _ in range(num_convs):\n",
        "        x = Conv2D(filters, kernel_size, strides=stride, padding=padding, \n",
        "                   kernel_regularizer=l2(weight_decay), kernel_initializer=initializer)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Define the VGG-19 architecture\n",
        "def create_vgg19_model(input_shape=(224, 224, 3), num_classes=6, weight_decay=1e-4, dropout_rate=0.5):\n",
        "    initializer = HeNormal(seed=42)  # Use Kaiming (HeNormal) initialization\n",
        "    # Define input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Block 1: 2 conv layers with 64 filters\n",
        "    x = make_layer(inputs, filters=64, num_convs=2)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
        "\n",
        "    # Block 2: 2 conv layers with 128 filters\n",
        "    x = make_layer(x, filters=128, num_convs=2)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
        "\n",
        "    # Block 3: 4 conv layers with 256 filters\n",
        "    x = make_layer(x, filters=256, num_convs=4)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
        "\n",
        "    # Block 4: 4 conv layers with 512 filters\n",
        "    x = make_layer(x, filters=512, num_convs=4)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
        "\n",
        "    # Block 5: 4 conv layers with 512 filters\n",
        "    x = make_layer(x, filters=512, num_convs=4)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
        "\n",
        "    # Flatten and fully connected layers\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(4096, activation='relu', kernel_regularizer=l2(weight_decay), kernel_initializer=initializer)(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(4096, activation='relu', kernel_regularizer=l2(weight_decay), kernel_initializer=initializer)(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax', kernel_regularizer=l2(weight_decay), kernel_initializer=initializer)(x)\n",
        "\n",
        "    # Build and return the model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Create the VGG-19 model\n",
        "weight_decay = 1e-4  # Set weight decay for regularization\n",
        "model = create_vgg19_model(input_shape=(224, 224, 3), num_classes=6, weight_decay=weight_decay, dropout_rate=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRroa33ckXXN"
      },
      "outputs": [],
      "source": [
        "# Compile the model with optimizer, loss function, and metrics\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "initial_learning_rate = 0.0001\n",
        "optimizer = AdamW(learning_rate=initial_learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=CategoricalCrossentropy(),\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "092rLMklkXSK"
      },
      "outputs": [],
      "source": [
        "# Train the model with callbacks for early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=150,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.save_weights('model.weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r8aT8x2kyQ7"
      },
      "source": [
        "# plot your Training Accuracy and validation accuracy here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEw_f7kwj2eC",
        "outputId": "91288895-66fd-4434-85f5-3a4da6611695"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train_acc', 'Val_acc'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train_loss', 'Val_loss'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUmB_6F-k5Wg"
      },
      "source": [
        "# Test your test data and save your result file in this format and upload your solution file on kaggle competition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmQO8SM7j2eC",
        "outputId": "6d38cc29-dbc1-42e6-c7a6-f69aa1f8794e"
      },
      "outputs": [],
      "source": [
        "test_probabilities = model.predict(test_generator)\n",
        "test_filenames = test_generator.filenames\n",
        "test_probabilities_df = pd.DataFrame(test_probabilities, columns=['a_Good', 'b_Moderate', \n",
        "                                                                  'c_Unhealthy_for_Sensitive_Groups', 'd_Unhealthy', \n",
        "                                                                  'e_Very_Unhealthy', 'f_Severe'])\n",
        "test_probabilities_df.insert(0, 'Filename', test_filenames)\n",
        "test_probabilities_df.to_csv('Outputs/test_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amBFm5hMj2eD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "\n",
        "# Generate predictions for the test data\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Get the true labels and predicted probabilities\n",
        "y_true = test_generator.classes\n",
        "y_pred_prob = model.predict(test_generator)\n",
        "\n",
        "# Calculate the F1 score for each class\n",
        "f1_scores = f1_score(y_true, y_pred, average=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUydKUwyj2eE",
        "outputId": "b7e43902-3f4a-4d6b-d4a4-a86ef84b13ed",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Calculate the F1 score for each class\n",
        "f1_scores = f1_score(y_true, y_pred, average=None)\n",
        "\n",
        "# Calculate the average F1 score\n",
        "average_f1_score = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Print the F1 scores for each class\n",
        "for i, f1 in enumerate(f1_scores):\n",
        "    print(\"Class {}: F1 Score = {:.4f}\".format(i, f1))\n",
        "\n",
        "# Print the average F1 score\n",
        "print(\"Average F1 Score: {:.4f}\".format(average_f1_score))\n",
        "print()\n",
        "print(\"Test Accuracy: {:.4f}\".format(test_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF7389o6j2eG",
        "outputId": "6e64690c-b75f-4eaf-efdb-e13082c6d3b1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Binarize the true labels\n",
        "y_true_binarized = label_binarize(y_true, classes=list(range(len(test_generator.class_indices))))\n",
        "\n",
        "# Calculate the ROC AUC score for each class\n",
        "roc_auc_scores = roc_auc_score(y_true_binarized, y_pred_prob, average='micro')\n",
        "\n",
        "# Plot the ROC curve for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(len(test_generator.class_indices)):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_prob[:, i])\n",
        "    roc_auc[i] = roc_auc_score(y_true_binarized[:, i], y_pred_prob[:, i])\n",
        "\n",
        "# Plot the ROC curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(len(test_generator.class_indices)):\n",
        "    plt.plot(fpr[i], tpr[i], label='Class %d (AUC = %0.2f)' % (i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Plot the random chance curve\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Add micro-average ROC AUC score to the plot\n",
        "plt.text(0.555, 0.34, 'Micro-average ROC AUC = %0.2f' % roc_auc_scores, fontsize=12, bbox=dict(facecolor='white', edgecolor='black'))\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBEdEolWj2eG",
        "outputId": "be73cb1b-91db-4c9b-ed0f-4bde740aa32b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, confusion_matrix\n",
        "\n",
        "# Generate predictions for the training data\n",
        "train_pred = model.predict(train_generator)\n",
        "train_pred = np.argmax(train_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Generate predictions for the validation data\n",
        "val_pred = model.predict(val_generator)\n",
        "val_pred = np.argmax(val_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Generate predictions for the testing data\n",
        "test_pred = model.predict(test_generator)\n",
        "test_pred = np.argmax(test_pred, axis=1)  # Convert predictions to class labels\n",
        "\n",
        "# Calculate overall MAE and RMSE\n",
        "train_mae = mean_absolute_error(train_generator.classes, train_pred)\n",
        "train_rmse = np.sqrt(mean_squared_error(train_generator.classes, train_pred))\n",
        "\n",
        "val_mae = mean_absolute_error(val_generator.classes, val_pred)\n",
        "val_rmse = np.sqrt(mean_squared_error(val_generator.classes, val_pred))\n",
        "\n",
        "test_mae = mean_absolute_error(test_generator.classes, test_pred)\n",
        "test_rmse = np.sqrt(mean_squared_error(test_generator.classes, test_pred))\n",
        "\n",
        "print(\"Overall MAE - Training: {:.4f}\".format(train_mae))\n",
        "print(\"Overall RMSE - Training: {:.4f}\".format(train_rmse))\n",
        "print(\"Overall MAE - Validation: {:.4f}\".format(val_mae))\n",
        "print(\"Overall RMSE - Validation: {:.4f}\".format(val_rmse))\n",
        "print(\"Overall MAE - Testing: {:.4f}\".format(test_mae))\n",
        "print(\"Overall RMSE - Testing: {:.4f}\".format(test_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBMYk8VKj2eH",
        "outputId": "a2792782-9e79-4fc6-ba6d-8e351d79368b",
        "scrolled": false
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。\n",
            "\u001b[1;31m請檢閱儲存格中的程式碼，找出失敗的可能原因。\n",
            "\u001b[1;31m如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。\n",
            "\u001b[1;31m如需詳細資料，請檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>。"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate confusion matrices\n",
        "train_cm = confusion_matrix(train_generator.classes, train_pred)\n",
        "val_cm = confusion_matrix(val_generator.classes, val_pred)\n",
        "test_cm = confusion_matrix(test_generator.classes, test_pred)\n",
        "\n",
        "# Define class labels\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Plot confusion matrix for training set\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(train_cm, annot=True, cmap='Blues', fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.title('Confusion Matrix - Training')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot confusion matrix for validation set\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(val_cm, annot=True, cmap='Greens', fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.title('Confusion Matrix - Validation')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot confusion matrix for testing set\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(test_cm, annot=True, cmap='Reds', fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.title('Confusion Matrix - Testing')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout() \n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
